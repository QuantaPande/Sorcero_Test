{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorcero Test\n",
    "## Tanmayan Pande\n",
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block contains the import statements used in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words obtained from: https://gist.github.com/sebleier/554280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The basic idea behind this text summarizer is that we can generate sentence embeddings by taking a weighted sum of the glove word embeddings of each word in the input text file. Since we have just a single, small text file, I am only taking the frequency with which each word occurs, after removing the stop words in the text file, as the weight of that word. For text corpuses with multiple documents, TF-IDF weights might be a better solution.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Strip each sentence of punctuations and convert it into lowercase\n",
    "2. For each word in the sentence, get the embeddings of the word according to pre-trained glove embeddings\n",
    "3. Calculate the normalized frequency of the word. The normalized frequency is defined as the frequency of the word divided by the frequency of the word which occurs most in the text document.\n",
    "4. Calculate the sentence embedding by using the formula \\begin{equation*} weight(sent) = \\sum_{i = 1}^n freq_{norm}(word) \\times embedding(word) \\end{equation*} where $n$ is the number of words in that sentence.\n",
    "5. Based on what percent of the text we want as a summary, we calculate the number of clusters that we need to divide the text into.\n",
    "6. We then fit a K-Means clustering model based on the number of clusters thus calculated and the sentence embeddings we calculated earlier\n",
    "7. Based on the means of the K-Means clustering, we find the closest sentence in each cluster and return the sentence.\n",
    "8. Then, we order these sentences based on the sentence order in the original text and return the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class ```KMeans```:\n",
    "\n",
    "An implementation of the KMeans clustering algorithm\n",
    "\n",
    "##### Constructor:\n",
    "Variables:\n",
    "1. ```data``` : The data variable which contains the data to be clustered\n",
    "2. ```clusters``` : The number of clusters to be fitted\n",
    "\n",
    "Description: Initalizes a KMeans model\n",
    "\n",
    "##### ```fit()```\n",
    "Variables:\n",
    "None\n",
    "\n",
    "Description: Fits the KMeans model on the data. This will run till the means do not change for one iteration\n",
    "Returns: The cluster means\n",
    "\n",
    "##### ```Euclidean_dist(point, array)```\n",
    "Variables:\n",
    "1. ```point``` : The first set of points\n",
    "2. ```array``` : The second set of points\n",
    "\n",
    "Description: Finds the Euclidean Distance between each row of two matrices\n",
    "Returns: The Euclidean distance between the point and the array\n",
    "\n",
    "##### ```get_closest_point(means)```\n",
    "Variables:\n",
    "1. ```means```: The means of the clusters in the KMeans model\n",
    "\n",
    "Description: Returns the point closest to the mean in each cluster\n",
    "Returns : The point closest to the mean in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    def __init__(self, data, clusters):\n",
    "        self.data = data\n",
    "        self.clusters = clusters\n",
    "\n",
    "    def Euclidean_dist(self, point, array):\n",
    "        \n",
    "        # new_mat is the difference between the point and the array\n",
    "        new_mat = array - point\n",
    "        dist_mat = np.zeros(new_mat.shape[0])\n",
    "        \n",
    "        # For each point in the set of points, calculate te distance\n",
    "        # of the point from all points in the data matrix\n",
    "        for i in range(0, new_mat.shape[0]):\n",
    "            dist_mat[i] = np.matmul(new_mat[i, :].T, new_mat[i, :])\n",
    "        \n",
    "        # Return the distance matrix\n",
    "        return dist_mat\n",
    "\n",
    "    def fit(self):\n",
    "        \n",
    "        # Declare the storage variables and initialize the means using a random distribution\n",
    "        dims = self.data.shape[1]\n",
    "        means = np.zeros((self.clusters, dims))\n",
    "        new_means = np.mean(self.data, axis = 0) + np.random.randn(self.clusters, dims) * np.std(self.data, axis = 0)\n",
    "        \n",
    "        # While the current means and new means are not the same, do\n",
    "        while np.all(Euclidean_dist(means, new_means) != 0):\n",
    "            means = new_means\n",
    "            clusters_ = [[] for i in range(self.clusters)]\n",
    "            \n",
    "            # Calculate the distance of the point from the cluster mean for all clusters\n",
    "            dist_mat = np.zeros((self.clusters, self.data.shape[0]))\n",
    "            for i in range(0, self.clusters):\n",
    "                dist_mat[i, :] = Euclidean_dist(means[i, :], self.data)\n",
    "                \n",
    "            # Get the cluster for which the distance is minimum\n",
    "            min_index = np.argmin(dist_mat, axis = 0)\n",
    "            \n",
    "            # Rearrange the clusters\n",
    "            for i in range(min_index.shape[0]):\n",
    "                clusters_[min_index[i]].append(self.data[i])\n",
    "            \n",
    "            # Calculate the new means based on the clusters\n",
    "            for i in range(self.clusters):\n",
    "                if len(clusters_[i]) != 0:\n",
    "                    new_means[i, :] = np.mean(clusters_[i], axis = 0)\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        # Return the means\n",
    "        return new_means\n",
    "    \n",
    "    def get_closest_point(self, means):\n",
    "        index = []\n",
    "        \n",
    "        # For all points in the data, check the point with minimum distance from\n",
    "        # each cluster mean\n",
    "        for i in range(0, means.shape[0]):\n",
    "            dist_mat = Euclidean_dist(self.data, means[i, :])\n",
    "            index.append(int(np.argmin(dist_mat)))\n",
    "        \n",
    "        # Return the indices of these points\n",
    "        return np.asarray(index, dtype = 'int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class ```Summarizer```:\n",
    "\n",
    "##### Constructor:\n",
    "Variables:\n",
    "1. ```text_path``` : The path of the text file to be summarized\n",
    "2. ```embedding_dims``` : The embedding size for the words. Valid values are 50, 100, 200, 300\n",
    "3. ```stop_words``` : Boolean flag indicating whether to keep stop words, or remove them. ```True``` signifies remove stop words\n",
    "4. ```summary_percent``` : The percent reduction in the text required. If between 0 and 1, the number is assumed to be a fraction of the summary. If greater than 1, the number is assumed to be the number of words in the summary.\n",
    "\n",
    "##### ```gen_embeddings()```:\n",
    "Variables:\n",
    "None\n",
    "\n",
    "Description: Constructs the word embeddings based on the pre-trained glove embeddings\n",
    "\n",
    "##### ```format_text()```:\n",
    "Variables:\n",
    "None\n",
    "\n",
    "Description: Formats the text as per the requirements of the model (converts to lowercase, strips punctuation)\n",
    "Returns : The formatted text\n",
    "\n",
    "##### ```gen_count(input_lines)```:\n",
    "\n",
    "Variables:\n",
    "1. ```input_lines``` : The set of sentences formatted according to the requirements of the model\n",
    "\n",
    "Description: Generates the word frequency dictionary and the final input sentence combination\n",
    "\n",
    "##### ```get_sentence_embeddings()```:\n",
    "Variables:\n",
    "None\n",
    "\n",
    "Description: Generates the sentence embeddings according to the formula given above\n",
    "Returns: The sentence embeddings\n",
    "\n",
    "##### ```get_sentences()``` :\n",
    "Variables:\n",
    "None\n",
    "\n",
    "Description: Generates the final summary by fitting a KMeans model on the sentence embeddings from the previous model\n",
    "Returns: The summarized version of the text\n",
    "\n",
    "##### ```Summarize()``` :\n",
    "Variables:\n",
    "None\n",
    "\n",
    "Description: Wrapper function for all the above functionality\n",
    "Returns: The summarized version of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer():\n",
    "    def __init__(self, text_path, embedding_dims = 50, stop_words = True, summary_percent = 0.2):\n",
    "        self.embedding_dims = embedding_dims\n",
    "        src = os.getcwd()\n",
    "        \n",
    "        # Open the input text and read the input\n",
    "        with open(src + \"/\" + text_path, \"r\", encoding = 'utf8') as file:\n",
    "            self.input_text = file.readline()\n",
    "        \n",
    "        # If stop words are to be removed, open and store the stop words\n",
    "        if stop_words:\n",
    "            with open(src + \"/stop_words.txt\", \"r\", encoding = 'utf8') as file:\n",
    "                self.stop_words = file.readlines()\n",
    "        self.stop_flag = stop_words\n",
    "        n_words = self.input_text.split(\" \")\n",
    "        \n",
    "        # Check the summary percent. It can't be negative or greater than the number of \n",
    "        # words in the text (n_words). If the summary percent is a number between 1 and \n",
    "        # n_words, it is converted into a fraction by dividing by the number of words \n",
    "        # in the text\n",
    "        if summary_percent < 0:\n",
    "            raise ValueError(\"The fraction of text cannot be a negative value\")\n",
    "        elif summary_percent > 0 and summary_percent <= 1:\n",
    "            self.percent_summary = summary_percent\n",
    "        elif summary_percent > 1 and summary_percent <= n_words: \n",
    "            self.percent_summary = summary_percent / n_words\n",
    "        else:\n",
    "            raise ValueError(\"The number of words in the summary can't be greater than the number of words in the text\")\n",
    "\n",
    "    def gen_embeddings (self):\n",
    "        self.embeddings = {}\n",
    "        \n",
    "        # Open the pre-trained embeddings and store them in a dictionary for easy access\n",
    "        with open(src + \"/glove.6B.{}d.txt\".format(self.embedding_dims), \"r\", encoding = 'utf8') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in tqdm(lines, desc = \"Loading Embeddings...\"):\n",
    "                line = line.strip()\n",
    "                split_ = line.split(\" \")\n",
    "                self.embeddings[split_[0]] = np.asarray(split_[1:]).astype('float32')\n",
    "\n",
    "    def format_text(self):\n",
    "        if self.stop_flag:\n",
    "            self.stop_words = [stop_word.strip() for stop_word in self.stop_words]\n",
    "            \n",
    "        # We use a regex to split the sentences, as there might be abbreviations in the text \n",
    "        # which might be unnecessarily split is we use the string.split method\n",
    "        pat = r'\\.\\s'\n",
    "        self.input_text = re.split(pat, self.input_text)\n",
    "        \n",
    "        # We then strip each line for empty spaces at the start and end,\n",
    "        # convert each line to lower case and \n",
    "        # strip any punctuations from the line\n",
    "        input_lines = [line.strip() for line in self.input_text]\n",
    "        input_lines = [line.lower() for line in input_lines]\n",
    "        input_lines = [line.translate(line.maketrans(\"\", \"\", string.punctuation)) for line in input_lines]\n",
    "        return input_lines\n",
    "    \n",
    "    def gen_count(self, input_lines):\n",
    "        self.word_count = {}\n",
    "        self.rectified_input = []\n",
    "        \n",
    "        # For each line in the input line,\n",
    "        for line in input_lines:\n",
    "            \n",
    "            # We split the line into words\n",
    "            for word in line.split(\" \"):\n",
    "                \n",
    "                # If we are removing stop words, \n",
    "                # ignore such words in the text\n",
    "                if self.stop_flag:\n",
    "                    if word in stop_words:\n",
    "                        continue\n",
    "                    # If the word is already in the\n",
    "                    # dictionary, increase its count\n",
    "                    if word in self.word_count:\n",
    "                        self.word_count[word] += 1\n",
    "                    \n",
    "                    # Else, add the word in the dictionary\n",
    "                    else:\n",
    "                        self.word_count[word] = 1\n",
    "                else:\n",
    "                    if word in self.word_count:\n",
    "                        self.word_count[word] += 1\n",
    "                    else:\n",
    "                        self.word_count[word] = 1\n",
    "            self.rectified_input.append(line)\n",
    "    \n",
    "    def get_sentence_embeddings(self):\n",
    "        \n",
    "        # Get the word which occurs most often in the text\n",
    "        max_freq = max(list(self.word_count.values()))\n",
    "        \n",
    "        # Normalize the word frequency by dividing by the max frequency\n",
    "        self.word_count = { word : values / max_freq \n",
    "                      for word, values in zip(list(self.word_count.keys()), list(self.word_count.values())) }\n",
    "\n",
    "        sent_weights = {}\n",
    "        \n",
    "        # For every sentence in the rectified input\n",
    "        for line in self.rectified_input:\n",
    "            \n",
    "            # The sentence embedding is a vector of zeroes initially\n",
    "            weight = np.zeros(self.embedding_dims)\n",
    "            \n",
    "            # For each word in the sentence\n",
    "            for word in line.split(\" \"):\n",
    "                try:\n",
    "                    \n",
    "                    # Add the weighted embedding of the word to the sentence \n",
    "                    # embeddings\n",
    "                    weight += self.word_count[word] * self.embeddings[word]\n",
    "                except:\n",
    "                    \n",
    "                    # If the word is not in the dictionary (i.e. it is a stop word),\n",
    "                    # ignore it\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "            # Normalize the sentence embedding by the length of the sentence.\n",
    "            # This is to ensure that longer sentences are not preferred\n",
    "            weight = weight / len(line.split(\" \"))\n",
    "            sent_weights[self.rectified_input.index(line)] = weight\n",
    "        \n",
    "        # Convert the dictionary of sentence embeddings into an array\n",
    "        # and return it\n",
    "        sent_weights_array = np.asarray([value for value in list(sent_weights.values())])\n",
    "        return sent_weights_array\n",
    "    \n",
    "    def get_sentences(self, sent_weights):\n",
    "        percent_summary = self.percent_summary\n",
    "        \n",
    "        # Based on the percent reduction, get the number of clusters\n",
    "        num_sent = int(percent_summary * len(sent_weights))\n",
    "        \n",
    "        # Declare a KMeans model and initialize it\n",
    "        model = KMeans(sent_weights, num_sent)\n",
    "        clusters = model.fit()\n",
    "        \n",
    "        # Get the sentences closest to the cluster means\n",
    "        indices = model.get_closest_point(clusters)\n",
    "        \n",
    "        # Sort these indices so as to maintain a coherent meaning\n",
    "        indices.sort()\n",
    "        \n",
    "        # Get the actual sentences from the original text\n",
    "        summary = [self.input_text[index] for index in indices]\n",
    "        \n",
    "        # Return the summary\n",
    "        return \". \".join(summary)\n",
    "    \n",
    "    def Summarize(self):\n",
    "        \n",
    "        # Just a wrapper function with the flow of the model\n",
    "        self.gen_embeddings()\n",
    "        input_lines = self.format_text()\n",
    "        self.gen_count(input_lines)\n",
    "        sent_weights = self.get_sentence_embeddings()\n",
    "        summary = self.get_sentences(sent_weights)\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Embeddings...: 100%|████████████████████████████████████████████████| 400001/400001 [00:17<00:00, 22241.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After creating the Mark II armor, Stark uploaded JARVIS into all of the Iron Man Armors, as well as allowing him to interact with the other Avengers, giving them valuable information during combat. JARVIS' duties were then taken over by FRIDAY.\n"
     ]
    }
   ],
   "source": [
    "summarizer = Summarizer(\"input.txt\", embedding_dims = 50, summary_percent = 0.2)\n",
    "print(summarizer.Summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses and Possible Solutions:\n",
    "\n",
    "1. The model only picks sentences from the original text in the summary. As such, the summary might not be a great representative of the ideas in the original text.\n",
    "2. We don't have a lot of freedom in number of words in the summary.\n",
    "\n",
    "The solution to these problems is to build a deep learning based text summarization tool. Using encoder decoder RNNs (or LSTMs), we can generate shorter versions of the text, which better capture the essence of the text. This is albeit a supervised technique, and hence will require a huge amount of training data to generate good results. This was the only reason I did not choose to build a deep learning based model, instead preferring a simple unsupervised model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
